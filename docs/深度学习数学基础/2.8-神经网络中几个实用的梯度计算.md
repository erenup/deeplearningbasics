[返回首页](../README.md)

## 2.8-神经网络中几个实用的梯度计算

2.8章节我们知道了张量求梯度的方法以及张量中的链式法则，这一章节我们结合Jacobian矩阵继续学习神经网络中几个非常使用的梯度计算。

### 2.8.1 $z = Wx$矩阵$W \in \mathbb R^{n \times m}$乘以列向量$x \in \mathbb R^m$求$\frac{\partial z}{\partial x}$

可以看作函数将输入$x \in \mathbb R^m$ 经过$W \in \mathbb R^{n \times m}$变换得到输出$z \in \mathbb R^n$,那么Jacobian矩阵$\frac{\partial z}{\partial x} \in \mathbb R^{n \times m}$

$$z_i = \sum^{m}_k W_{ik}x_k$$

那么
$$(\frac{\partial z}{\partial x})_{i,j} = \frac{\partial z_i}{\partial x_j} = \frac{\partial {\sum^{m}_k W_{ik}x_k}}{\partial x} = \sum^{m}_k W_{ik} \frac{\partial x_k}{\partial x_j} = W_{ij}$$

由于$\frac{\partial x_k}{\partial x_j} = 1$ if $k = j$ else 0， 所以有$\frac{\partial z}{\partial x} = W$

### 2.8.2 $z=xW, \frac{\partial z}{\partial x} = W^T$

### 2.8.3 $z=x$向量等于自身,求$\frac{\partial z}{\partial x}$

因为$z_i = x_i$ 所以
$$(\frac{\partial z}{\partial x})_{i,j} = \frac{\partial z_i}{\partial x_j} = \frac{\partial x_i}{\partial x_j} = \{^{1, i=j}_{0, otherwise}$$

所以$\frac{\partial z}{\partial x} = I$,将其放在链式法则中进行矩阵乘法时候不会改变其他矩阵。

### 2.8.4 $z = f(x)$ 对向量$x$中每个元素进行变换, 求$\frac{\partial z}{\partial x}$
由于$z_i = f(x_i)$所以
$$(\frac{\partial z}{\partial x})_{i,j} = \frac{\partial z_i}{\partial x_j} = \frac{\partial f(x_i)}{\partial x_j} = \{^{f'(x_i), i=j}_{0, otherwise}$$

所以$\frac{\partial z}{\partial x}$是一个diagonal matrix 且$\frac{\partial z}{\partial x} = diag(f'(x))$

矩阵乘以一个diagonal矩阵也就是每个元素进行幅度变换，因此链式法则中的矩阵乘以$diag(f'(x))$相当于和$f'(x)$做elementwise 乘法。

### 2.8.5 $z = Wx, \delta = \frac{\partial J}{\partial z}$，求$\frac{\partial J}{\partial W}=\frac{\partial J}{\partial z} \frac{\partial z}{\partial W}=\delta \frac{\partial z}{\partial W}$

我们开始引入更复杂的情况，因为神经网络中往往包含多次链式法则的引用，这里我们假设已经知道$\delta = \frac{\partial J}{\partial z}$，直接求$\frac{\partial J}{\partial W}$。

假设神经网络的损失函数$J$是标量，我们想计算的是损失函数对参数$W \in \mathbb R^{n \times m}$的梯度。我们可以想象神经网络这个函数输入是一个$n \times m$形状的参数，输出是一个标量，结合上一章节Jacobian知识我们可以知道$\frac{\partial J}{\partial W} \in \mathbb R^{(1) \times (n \times m)}$形状和$W$一样，所以在神经网络训练的时候可以将参数减轻去参数的梯度乘以学习率。

根据链式法则，我们需要求出$\frac{\partial z}{\partial W} \in \mathbb R^{(n) \times (n \times m)}$。这个三维的张量不方便表示且十分复杂，因此我们先只看对$W_{i,j}$求导$\frac{\partial z}{\partial W_{i,j}} \in \mathbb R^{(n) \times (1)}$。

$$z_k = \sum^m_{l=1}  W_{kl}x_l$$
$$\frac{\partial z}{\partial W_{ij}} = \sum^m_{l=1}x_l \frac{\partial W_{kl}}{\partial W_{ij}}$$

$$\frac{\partial W_{kl}}{\partial W_{ij}} = \{^{1, i=k, j=l}_0$$

所以只有$i=k, j=l$时候非零
$$\frac{\partial J}{\partial W_{ij}} = \begin{bmatrix}
0\\ 
.\\ 
x_j\\ 
.\\ 
0
\end{bmatrix} \leftarrow  ith element, j=l$$

所以
$$\frac{\partial J}{\partial W_{ij}} = \frac{\partial J}{\partial z}\frac{\partial z}{\partial W_{ij}} = \delta \frac{\partial z}{\partial W_{ij}} = \delta_i x_j$$

所以得到
$$\frac{\partial J}{\partial W} = \delta^T x^T $$

### 2.8.6 $z=xW, \delta = \frac{\partial J}{\partial z}, \frac{\partial J}{\partial W} =x^T\delta$

### 2.8.7 $\hat{y} =  softmax(\Theta)， J=Cross-entropy(y,\hat{y}), \frac{\partial J}{\partial \Theta } = \hat{y} - y$

假设神经网络到达softmax之前的输出为$\Theta = [\theta_1, \theta_2, ..., \theta_n]$,$n$为分类数量，那么
$$\hat{y} = softmax(\Theta), \theta_i = \frac{e^{\theta_i}}{\sum^n_k e^{\theta_k}}$$
$$J = \sum^n_{i=1}y_ilog(\hat{y_i})$$
$$\frac{\partial J}{\hat{y_i}} = \frac{y_i}{\hat{y_i}}$$
$$\frac{\partial \hat{y_i}}{\theta_i} = \{^{\hat{y_i}(1-\hat{y_k}), i=k}_{-\hat{y_i}\hat{y_k}, i \neq k}$$

$$\frac{\partial J}{\theta_i} = \frac{\partial J}{\hat{y}} \frac{\partial \hat{y}}{\theta_i}, 形状为1 \times n, n \times 1$$

$$\frac{\partial J}{\theta_i} = \sum^n_{k=1}{\frac{\partial J}{\hat{y_k}} \frac{\partial \hat{y_k}}{\theta_i}} (k=i, k\neq i)$$

$$\frac{\partial J}{\theta_i} = \frac{\partial J}{\hat{y_i}} \frac{\partial \hat{y_i}}{\theta_i} + \sum^n_{k \neq  i}{\frac{\partial J}{\hat{y_k}} \frac{\partial \hat{y_k}}{\theta_i}} (k=i, k\neq i)$$

$$\frac{\partial J}{\theta_i} = y_i(1- \hat{y_i}) + \sum_{k neq i} {y_k \hat{y_i}}$$

$$\frac{\partial J}{\theta_i} = -y_i(1- \hat{y_i}) + \sum_{k neq i} {y_k \hat{y_i}}$$

$$\frac{\partial J}{\theta_i} = -y_i + \hat{y_i} \sum {y_k} $$

$$\frac{\partial J}{\theta_i} = \hat{y_i} -y_i  $$

所以
$$\frac{\partial J}{\theta} = \hat{y} -y \in \mathbb R^{n}  $$