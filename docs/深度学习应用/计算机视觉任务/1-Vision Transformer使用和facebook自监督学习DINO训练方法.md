# å»ºè®®åœ¨è°·æ­Œcolabä¸Šä½¿ç”¨å…è´¹çš„GPUè¿è¡Œæœ¬Notebookï¼Œå°‘å®‰è£…å¾ˆå¤šä¾èµ–ã€‚

[google colabä¸­æ‰“å¼€](https://drive.google.com/file/d/1Qd4gpc5Vk_YvRVsCd9K7XdV8yxrhBSXA/view?usp=sharing)

æœ¬æ–‡ä¸»è¦å†…å®¹æœ‰ï¼š
1. FacebookåŸºäºVision Transformersçš„è‡ªç›‘ç£ç ”ç©¶DINOç›¸å…³æ¨¡å‹åœ¨è§†é¢‘ä¸ŠæŠ½å–featureå¹¶å±•ç¤ºattention map
2. Huggingface/Transformersä¸­Vision Transformersçš„åŸºæœ¬ä½¿ç”¨æ–¹æ³•ã€‚


æœ¬æ–‡ä¸»è¦å‚è€ƒèµ„æ–™æ˜¯ï¼š


*  https://gist.github.com/aquadzn/32ac53aa6e485e7c3e09b1a0914f7422
*   https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb
* https://arxiv.org/pdf/2104.14294.pdf
* https://arxiv.org/abs/2010.11929



## facebook DINOåœ¨è§†é¢‘ä¸Šçš„å°è¯•å°è¯•
## æ•°æ®/ä»£ç å‡†å¤‡


```python
# å»ºè®®åŠ è½½è‡ªå·±çš„google driveæ–¹ä¾¿ä¸Šä¼ è‡ªå®šä¹‰è§†é¢‘è¿›è¡Œå°è¯•ã€‚
from google.colab import drive
drive.mount('/content/drive')
```

    Mounted at /content/drive



```python
%cd /content/drive/MyDrive/transformer_research
#åˆ‡æ¢æˆä½ çš„æ–‡ä»¶å¤¹ï¼Œcolabå·¦è¾¹æœ‰ä¸ªå‘ä¸Šçš„ç®­å¤´ï¼Œæ‰¾åˆ°/content/ç›®å½•ä¸‹ä½ çš„ç›®å½•ï¼Œç„¶åå³é”®å¤åˆ¶è·¯å¾„
```

    /content/drive/MyDrive/transformer_research



```python
!pwd
!mkdir input
!mkdir output
```

    /content/drive/MyDrive/transformer_research
    mkdir: cannot create directory â€˜inputâ€™: File exists
    mkdir: cannot create directory â€˜outputâ€™: File exists



```python
!git clone https://github.com/facebookresearch/dino.git
# ä¸‹è½½DINOä»£ç åº“
```

    fatal: destination path 'dino' already exists and is not an empty directory.


Download a model, here I used deit small 8 pretrained


```python
!wget https://dl.fbaipublicfiles.com/dino/dino_deitsmall8_pretrain/dino_deitsmall8_pretrain.pth -O dino/dino_deitsmall8_pretrain.pth
#ä¸‹è½½æ¨¡å‹åˆ°å½“å‰ç›®å½•çš„dinoä¸‹çš„dino_deitsmall8_pretrain.pth
```

    --2021-05-03 14:24:26--  https://dl.fbaipublicfiles.com/dino/dino_deitsmall8_pretrain/dino_deitsmall8_pretrain.pth
    Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...
    Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 86728949 (83M) [application/zip]
    Saving to: â€˜dino/dino_deitsmall8_pretrain.pthâ€™
    
    dino/dino_deitsmall 100%[===================>]  82.71M  14.4MB/s    in 6.8s    
    
    2021-05-03 14:24:34 (12.2 MB/s) - â€˜dino/dino_deitsmall8_pretrain.pthâ€™ saved [86728949/86728949]
    


æ‰¾ä¸ªæ„Ÿå…´è¶£çš„è§†é¢‘ä¸‹è½½ä¸‹æ¥å¹¶ä¸Šä¼ åˆ°è¿™é‡Œï¼Œå‡è®¾åå­—æ˜¯bilibili_cat.mp4ï¼Œæœ€å¥½æ˜¯10sä»¥å†…ï¼Œå…è´¹çš„gpuç®—ä¸äº†å¤ªå¤šã€‚
è¿™é‡Œæœ‰ä¸ª[ä¾‹å­](https://www.pexels.com/fr-fr/video/chien-course-exterieur-journee-ensoleillee-4166347/)

ç„¶åç”¨ffmpegå°†è§†é¢‘è½¬åŒ–ä¸ºjpgï¼Œå‚æ•°æ˜¯60fpsï¼Œç„¶åå¦‚æœæ˜¯10ç§’çš„è¯ï¼Œå°±æ˜¯600å¼ ã€‚
Then you need to extract frames from the video, you can use ffmpeg.

Video is 60 fps and ~6 sec so you'll get ~360 jpg images

%03d is from 001 to 999


```python
!ffmpeg -i ./bilibili_cat.mp4 input/img-%03d.jpg
```



```python
%cd dino/
```

    /content/drive/MyDrive/transformer_research/dino


ç›¸å…³ä»£ç ï¼Œæ¥æºæ˜¯ï¼šhttps://gist.github.com/aquadzn/32ac53aa6e485e7c3e09b1a0914f7422

Requirements:


* Opencv
* scikit-image
* maptlotlib
* pytorch
* numpy
* Pillow




```python
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import os
import gc
import sys
import argparse
import cv2
import random
import colorsys
import requests
from io import BytesIO

import skimage.io
from skimage.measure import find_contours
import matplotlib.pyplot as plt
from matplotlib.patches import Polygon
import torch
import torch.nn as nn
import torchvision
from torchvision import transforms as pth_transforms
import numpy as np
from PIL import Image

import utils
import vision_transformer as vits
```

æ³¨æ„ï¼ï¼GPUå¤§å°æœ‰é™ï¼Œå¦‚æœè§†é¢‘åˆ†è¾¨ç‡å¤ªé«˜ï¼Œé‚£ä¹ˆæ¯å¼ å›¾éƒ½å¾ˆå¤§ï¼Œéœ€è¦resizeä¸€ä¸‹ï¼Œè¿™é‡Œæ˜¯resizeçš„512x512ï¼Œå¦‚æœOOMè·‘ä¸äº†å°±æ”¹å°ä¸€ç‚¹ã€‚

æ”¹è¿™é‡Œç¬¬9è¡Œï¼š`pth_transforms.Resize(512)`


```python
def predict_video(args):
    for frame in sorted(os.listdir(args.image_path)):
        with open(os.path.join(args.image_path, frame), 'rb') as f:
            img = Image.open(f)
            img = img.convert('RGB')

        transform = pth_transforms.Compose([
            pth_transforms.ToTensor(),
            pth_transforms.Resize(512),
            pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
        ])
        img = transform(img)

        # make the image divisible by the patch size
        w, h = img.shape[1] - img.shape[1] % args.patch_size, img.shape[2] - img.shape[2] % args.patch_size
        img = img[:, :w, :h].unsqueeze(0)

        w_featmap = img.shape[-2] // args.patch_size
        h_featmap = img.shape[-1] // args.patch_size

        attentions = model.forward_selfattention(img.cuda())

        nh = attentions.shape[1] # number of head

        # we keep only the output patch attention
        attentions = attentions[0, :, 0, 1:].reshape(nh, -1)

        # we keep only a certain percentage of the mass
        val, idx = torch.sort(attentions)
        val /= torch.sum(val, dim=1, keepdim=True)
        cumval = torch.cumsum(val, dim=1)
        th_attn = cumval > (1 - args.threshold)
        idx2 = torch.argsort(idx)
        for head in range(nh):
            th_attn[head] = th_attn[head][idx2[head]]
        th_attn = th_attn.reshape(nh, w_featmap, h_featmap).float()
        # interpolate
        th_attn = nn.functional.interpolate(th_attn.unsqueeze(0), scale_factor=args.patch_size, mode="nearest")[0].cpu().numpy()

        attentions = attentions.reshape(nh, w_featmap, h_featmap)
        attentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=args.patch_size, mode="nearest")[0].cpu().numpy()

        # save attentions heatmaps
        os.makedirs(args.output_dir, exist_ok=True)

        # Saving only last attention layer
        fname = os.path.join(args.output_dir, "attn-" + frame)
        plt.imsave(
            fname=fname,
            arr=sum(attentions[i] * 1/attentions.shape[0] for i in range(attentions.shape[0])),
            cmap="inferno",
            format="jpg"
        )
        print(f"{fname} saved.")
```


```python
#@title Args

pretrained_weights_path = "dino_deitsmall8_pretrain.pth" #@param {type:"string"}
arch = 'deit_small' #@param ["deit_small", "deit_tiny", "vit_base"]
input_path = "../input/" #@param {type:"string"}
output_path = "../output/" #@param {type:"string"}
threshold = 0.6 #@param {type:"number"}


parser = argparse.ArgumentParser('Visualize Self-Attention maps')
parser.add_argument('--arch', default='deit_small', type=str,
    choices=['deit_tiny', 'deit_small', 'vit_base'], help='Architecture (support only ViT atm).')
parser.add_argument('--patch_size', default=8, type=int, help='Patch resolution of the model.')
parser.add_argument('--pretrained_weights', default='', type=str,
    help="Path to pretrained weights to load.")
parser.add_argument("--checkpoint_key", default="teacher", type=str,
    help='Key to use in the checkpoint (example: "teacher")')
parser.add_argument("--image_path", default=None, type=str, help="Path of the image to load.")
parser.add_argument('--output_dir', default='.', help='Path where to save visualizations.')
parser.add_argument("--threshold", type=float, default=0.6, help="""We visualize masks
    obtained by thresholding the self-attention maps to keep xx% of the mass.""")

args = parser.parse_args(args=[])

args.arch = arch
args.pretrained_weights = pretrained_weights_path
args.image_path = input_path
args.output_dir = output_path
args.threshold = threshold
```


```python
model = vits.__dict__[args.arch](patch_size=args.patch_size, num_classes=0)
for p in model.parameters():
    p.requires_grad = False
model.eval()
model.cuda()
if os.path.isfile(args.pretrained_weights):
    state_dict = torch.load(args.pretrained_weights, map_location="cpu")
    if args.checkpoint_key is not None and args.checkpoint_key in state_dict:
        print(f"Take key {args.checkpoint_key} in provided checkpoint dict")
        state_dict = state_dict[args.checkpoint_key]
    state_dict = {k.replace("module.", ""): v for k, v in state_dict.items()}
    msg = model.load_state_dict(state_dict, strict=False)
    print('Pretrained weights found at {} and loaded with msg: {}'.format(args.pretrained_weights, msg))
else:
    print("Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.")
    url = None
    if args.arch == "deit_small" and args.patch_size == 16:
        url = "dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth"
    elif args.arch == "deit_small" and args.patch_size == 8:
        url = "dino_deitsmall8_300ep_pretrain/dino_deitsmall8_300ep_pretrain.pth"  # model used for visualizations in our paper
    elif args.arch == "vit_base" and args.patch_size == 16:
        url = "dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth"
    elif args.arch == "vit_base" and args.patch_size == 8:
        url = "dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth"
    if url is not None:
        print("Since no pretrained weights have been provided, we load the reference pretrained DINO weights.")
        state_dict = torch.hub.load_state_dict_from_url(url="https://dl.fbaipublicfiles.com/dino/" + url)
        model.load_state_dict(state_dict, strict=True)
    else:
        print("There is no reference weights available for this model => We use random weights.")

```

    Pretrained weights found at dino_deitsmall8_pretrain.pth and loaded with msg: <All keys matched successfully>



```python
torch.cuda.empty_cache()
gc.collect()
```




    268



## DINOæŠ½å–feature
ä½¿ç”¨DINOé¢„è®­ç»ƒå¥½çš„ViTå¯¹è§†é¢‘è½¬åŒ–ä¹‹åçš„å›¾ç‰‡æŠ½featuresï¼Œå¦‚æœOOMï¼ŒæŠŠä¸Šé¢çš„resizeå‚æ•°æ”¹å°ä¸€ç‚¹ã€‚


```python
predict_video(args)
```

## è¾“å‡ºè§†é¢‘

è¾“å…¥è§†é¢‘æ˜¯60å¸§æ¯ç§’ï¼Œè¾“å‡ºä¹Ÿæ˜¯ã€‚


```python
!ffmpeg -framerate 60 -i ../output/attn-img-%03d.jpg ../output.mp4
```

è¾“å…¥è¾“å‡ºå¯¹æ¯”æ”¾ä¸€èµ·ï¼Œè¾“å‡ºä¹‹åä»google driveä¸Šä¸‹è½½ä¸‹æ¥å°±å¯ä»¥äº†ã€‚


```python
!ffmpeg -i ../bilibili_cat.mp4 -i ../output.mp4 -filter_complex '[0:v]pad=iw*2:ih[int];[int][1:v]overlay=W/2:0[vid]' -map '[vid]' -c:v libx264 -crf 23 -preset veryfast ../final.mp4
```


# Huggingfaceé‡ŒViTçš„åŸºæœ¬ä½¿ç”¨

## åœ¨CIFAR-10ä¸Šå¾®è°ƒFine-tune ViTæ¨¡å‹
ä¸»è¦åŸºäºè¿™ä¸ª[notbook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb)ï¼Œè¿™ä¸ªå¤§ä½¬å†™çš„https://nielsrogge.github.io/ã€‚

ä»–åŸæœ¬çš„æœ¬åœ°æƒ³ç›´æ¥è·‘èµ·æ¥å…¶å®ç¼ºå°‘ä¸å°‘åº“ï¼Œéœ€è¦é¢å¤–å®‰è£…ä¸‹ã€‚

åœ¨è¿™ä¸ªnotebookä¸­æˆ‘ä»¬åŸºäºé¢„è®­ç»ƒçš„Vision Transformeråœ¨CIFAR-10ä¸Šåšä¸€ä¸ªåˆ†ç±»ä»»åŠ¡ã€‚CIFAR-10æ•°æ®é›†åŒ…å«äº†60000ä¸ª32x32çš„å½©è‰²å›¾ç‰‡ï¼Œæ€»å…±10ä¸ªç±»åˆ«ï¼Œæ¯ä¸ªç±»åˆ«6000å¼ å›¾ã€‚

ä½¿ç”¨Huggingfaceçš„[ğŸ¤— datasets](https://github.com/huggingface/datasets)é¢„å¤„ç†æ•°æ®ï¼Œä½¿ç”¨[ğŸ¤— Trainer](https://huggingface.co/transformers/main_classes/trainer.html)å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚


### ç®€å•ä»‹ç»: Vision Transformer (ViT) by Google Brain
vision TransformeråŸºæœ¬ä¸Šå’ŒBERTç›¸åŒï¼Œä¸åŒçš„åœ°æ–¹åœ¨äºç”¨åœ¨äº†å›¾åƒä¸Šã€‚ä¸ºäº†èƒ½å¤Ÿè®©transformerç”¨åœ¨å›¾åƒä¸Šï¼Œå®ƒå°†ä¸€å¼ å›¾åˆ‡åˆ†æˆå¤šä¸ªpatchesï¼ˆæƒ³è±¡æˆä¸€ç³»åˆ—ç½‘æ ¼å³å¯ï¼‰ï¼Œç„¶åå°†æ‰€æœ‰çš„patchesè¿æ¥èµ·æ¥çœ‹æˆåºåˆ—ï¼Œæ¯ä¸ªpatchå¯¹åº”nlpé‡Œçš„ä¸€ä¸ªtokenã€‚å’ŒNLPç›¸ä¼¼ï¼Œç›´æ¥åœ¨patchesåºåˆ—çš„å¼€å¤´æ·»åŠ ä¸€ä¸ª[CLS] tokenç”¨æ¥èšåˆæ•´ä¸ªå›¾ç‰‡çš„ä¿¡æ¯ï¼Œæ¯ä¸ªpatchï¼ˆtokenï¼‰å¾—åˆ°ä¸€ä¸ªembeddingï¼ŒåŒæ ·æ¯ä¸ªpatchä¹Ÿå¯¹åº”äº†ä¸€ä¸ªposition embedingï¼Œç„¶åæŠŠtoken çš„embeddingå’Œä½ç½®å‘é‡ä¸€èµ·é€å…¥transformeræ¨¡å‹å³å¯ã€‚ViTåœ¨åœ¨å›¾åƒä¸Šå–å¾—äº†å¾ˆå¥½çš„æ•ˆæœã€‚

* Paper: https://arxiv.org/abs/2010.11929
* Official repo (in JAX): https://github.com/google-research/vision_transformer


```python
!pip install -q git+https://github.com/huggingface/transformers datasets
```

      Installing build dependencies ... [?25l[?25hdone
      Getting requirements to build wheel ... [?25l[?25hdone
        Preparing wheel metadata ... [?25l[?25hdone
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 225kB 21.9MB/s 
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 49.6MB/s 
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 53.1MB/s 
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 245kB 59.4MB/s 
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112kB 61.7MB/s 
    [?25h  Building wheel for transformers (PEP 517) ... [?25l[?25hdone


## æ•°æ®å‡†å¤‡

è¿™é‡Œåªç”¨CIFAR-10æ•°æ®ä¸€éƒ¨åˆ†æ¥ä½œä¸ºæ¼”ç¤ºã€‚ä½¿ç”¨ `ViTFeatureExtractor`æŠ½å–å›¾ç‰‡ç‰¹å¾. `ViTFeatureExtractor`ä¼šå°†æ¯ä¸ª32x32çš„å›¾ç‰‡resizeæˆ224x224å¤§å°ï¼ŒåŒæ—¶å¯¹æ¯ä¸ªchannelè¿›è¡Œå½’ä¸€åŒ–ã€‚
æœ¬æ–‡ä¸»è¦æ˜¯æ¼”ç¤ºï¼Œæƒ³è¦æ›´å¥½çš„æ•ˆæœéœ€è¦æ›´å®Œæ•´æ•°æ®å’Œæ›´é«˜çš„å›¾ç‰‡åˆ†è¾¨ç‡è¿›è¡Œè®­ç»ƒã€‚


```python
from transformers import ViTFeatureExtractor

feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')
```


    HBox(children=(FloatProgress(value=0.0, description='Downloading', max=160.0, style=ProgressStyle(description_â€¦


    



```python
#åŠ è½½æ•°æ®
from datasets import load_dataset

# load cifar10 (only small portion for demonstration purposes) 
train_ds, test_ds = load_dataset('cifar10', split=['train[:5000]', 'test[:1000]'])
# split up training into training + validation
splits = train_ds.train_test_split(test_size=0.1)
train_ds = splits['train']
val_ds = splits['test']
```


```python
# æ•°æ®é¢„å¤„ç†å‡½æ•°
import numpy as np

def preprocess_images(examples):
    # get batch of images
    images = examples['img']
    # convert to list of NumPy arrays of shape (C, H, W)
    images = [np.array(image, dtype=np.uint8) for image in images]
    images = [np.moveaxis(image, source=-1, destination=0) for image in images]
    # preprocess and add pixel_values
    inputs = feature_extractor(images=images)
    examples['pixel_values'] = inputs['pixel_values']

    return examples
```


```python
# æ•°æ®é¢„å¤„ç†ï¼Œå¤§æ¦‚å‡ åˆ†é’Ÿ
from datasets import Features, ClassLabel, Array3D

# we need to define the features ourselves as both the img and pixel_values have a 3D shape 
features = Features({
    'label': ClassLabel(names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']),
    'img': Array3D(dtype="int64", shape=(3,32,32)),
    'pixel_values': Array3D(dtype="float32", shape=(3, 224, 224)),
})

preprocessed_train_ds = train_ds.map(preprocess_images, batched=True, features=features)
preprocessed_val_ds = val_ds.map(preprocess_images, batched=True, features=features)
preprocessed_test_ds = test_ds.map(preprocess_images, batched=True, features=features)
```



    


## å®šä¹‰æ¨¡å‹
å®šä¹‰ä¸€ä¸ªåˆ†ç±»æ¨¡å‹ï¼Œåœ¨ViTä¸Šé¢åŸºäºCLS tokenè¿‡ä¸€ä¸ªå…¨è¿æ¥ç½‘ç»œå³å¯ã€‚


```python
from transformers import ViTModel
from transformers.modeling_outputs import SequenceClassifierOutput
import torch.nn as nn

class ViTForImageClassification(nn.Module):
    def __init__(self, num_labels=10):
        super(ViTForImageClassification, self).__init__()
        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(self.vit.config.hidden_size, num_labels)
        self.num_labels = num_labels

    def forward(self, pixel_values, labels):
        outputs = self.vit(pixel_values=pixel_values)
        output = self.dropout(outputs.last_hidden_state[:,0])
        logits = self.classifier(output)

        loss = None
        if labels is not None:
          loss_fct = nn.CrossEntropyLoss()
          loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))

        return SequenceClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )
```


```python
# è¯„ä¼°æ–¹æ³•
from transformers import TrainingArguments, Trainer

metric_name = "accuracy"
# è®­ç»ƒå‚æ•°
args = TrainingArguments(
    f"test-cifar-10",
    evaluation_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=10,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model=metric_name,
    logging_dir='logs',
)
```


```python
from transformers import default_data_collator

data_collator = default_data_collator
```


```python
#å®šä¹‰æ¨¡å‹
model = ViTForImageClassification()
```


## è®­ç»ƒå’Œåˆ†æ


```python
# å¦‚ä½•è®¡ç®—åˆ†æ•°
from datasets import load_metric
import numpy as np

metric = load_metric("accuracy")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)
```


    HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1362.0, style=ProgressStyle(descriptionâ€¦


    



```python
# å®šä¹‰è®­ç»ƒæ¡†æ¶
trainer = Trainer(
    model,
    args,
    train_dataset=preprocessed_train_ds,
    eval_dataset=preprocessed_val_ds,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)
```


```python
# Start tensorboard.
%load_ext tensorboard
%tensorboard --logdir logs/
```


    <IPython.core.display.Javascript object>



```python
# å¼€å§‹è®­ç»ƒ
trainer.train()
```



    <div>

      <progress value='270' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [270/270 02:36, Epoch 3/3]
    </div>
    <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
      <th>Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>No log</td>
      <td>1.851132</td>
      <td>0.920000</td>
    </tr>
    <tr>
      <td>2</td>
      <td>No log</td>
      <td>1.523874</td>
      <td>0.950000</td>
    </tr>
    <tr>
      <td>3</td>
      <td>No log</td>
      <td>1.407741</td>
      <td>0.960000</td>
    </tr>
  </tbody>
</table><p>





    TrainOutput(global_step=270, training_loss=1.685737779405382, metrics={'train_runtime': 157.1564, 'train_samples_per_second': 1.718, 'total_flos': 0, 'epoch': 3.0, 'init_mem_cpu_alloc_delta': 1325400064, 'init_mem_gpu_alloc_delta': 345588224, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 589918208, 'train_mem_gpu_alloc_delta': 1054787072, 'train_mem_cpu_peaked_delta': 50122752, 'train_mem_gpu_peaked_delta': 1691880448})




```python
# é¢„æµ‹
outputs = trainer.predict(preprocessed_test_ds)
```



<div>

  <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>
  [250/250 00:22]
</div>




```python
# åˆ†æ
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

y_true = outputs.label_ids
y_pred = outputs.predictions.argmax(1)

labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
disp.plot(xticks_rotation=45)
```





    
![png](Vision%20Transformer%E4%BD%BF%E7%94%A8%E5%92%8Cfacebook%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0DINO%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95play_files/Vision%20Transformer%E4%BD%BF%E7%94%A8%E5%92%8Cfacebook%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0DINO%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95play_45_1.png)
    



```python

```
